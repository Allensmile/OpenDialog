## 检索式方法

1. 数据可否不使用平行语料，使用大量的非平行语料扩充
2. 使用知识图谱做检索式主题控制，保证多轮对话的一致性，流畅性
3. 使用知识图谱增强粗筛
4. 直接使用知识图谱套模板对话系统(无监督生成/符合对话上下文的文本生成)
5. 发现一个检索式系统的问题，检索式系统好像倾向于上下文主题的匹配程度，但是不考虑其他的方面，现有的检索式对话系统其实主要认为检索到的句子质量就一定好，但是不一定（比如，下面的句子仅仅是主题一致，但是并不流畅，不是一个好的回复，所以单纯使用bert通过负采样计算的并不是好的方法，可以参考IRGAN生成更高质量的负样本?甚至说用生成式增强检索式也是一个可以写论文的idea(Multiview的检索式对话模型))
    ```python
    Context: 你喜欢什么电影     # Coherence scores
    Reference: 我喜欢爱情片     # 0.2245
    Generated: 电影电影电影     # 0.1602
    Generated: 你喜欢什么电影   # 0.0949
    Generated: 爱情喜欢我电影   # 0.1945
    ```
6. 从这个角度说，multiview其实就是一个改良的检索模型，适合发论文
7. 不仅要从response抽负样本，还要把context作为负样本
8. Dialog Evaluation任务描述
    Dialog Evaluation是一个自然语言处理中一个非常重要的问题，目的是评价候选句子是否是针对对话上下文的合适回复。目前Dialog Evaluation有如下三个主要的应用场景：对话自动评估，检索式对话，基于强化学习的对话。目前这三个子方向中对对话评估都只从一个单一的角度对对话进行评价，但是单一一个分数无法有效的涵盖所有的评估要点。

    * 一致性Coherence: 判断下文是否是上文的回复，负采样训练
    * 流畅性Fluency: 主要针对生成式对话系统，判断对话生成的句子的流畅性（单词级），采用负采样训练[重复，删除，替换，语言模型计算]
    * 逻辑性Logicality: 判断句子的上下文逻辑是否连贯（句子级），负采样训练[句子重复，句子删除，句子打乱顺序]
    
9. 检索式方法的粗筛，Q-A匹配比Q-Q匹配好很多

## 生成式

1. GPT2(直接在 GPT2-chitchat 上开始 fine-tune?)
2. 检索加强的GPT2
3. 互助学习方式
4. 使用知识图谱提供facts知识，连同检索结果一起加强生成过程
5. 采样nbest个句子，通过Topic, fluency, appropriateness进行多样性打分最终来筛选一个合适的结果，这个打分的包不仅可以用来做线下评估，还可以用来作为GPT2生成模型的筛选模型，还可以作为奖励生成器(paper的idea)
    * 主题相关性: 训练主题分类模型(文本5分类问题：电影，美食，音乐，数码电子，体育)
    * 流畅度: 使用GPT2语言模型计算ppl和对应的safety公式
    * 多样性: 使用distinct指标计算distinct-1/2(需要分词)
    * 兼容性: 训练bert nli模型判断逻辑是否存在错误
    * 一致性: 检索模型(Learning-based metric)判断是否和上下文语义一致
6. 生成式一次生成多个句子，统一进行重排序

## 评价指标

1. 评价指标可以用来线下评测，并且也可以用来判断是否兜底，是非常有必要的
2. 可以把检索式对话系统直接拿来做基于学习的评价指标，可能会用到数据增强等方法进一步提升效果
3. Multi-view的评价指标可以有如下的应用：自动评估响应的效果，reranker，强化学习的奖励生成器

## Schedule

- [x] 完成API测试
- [x] 准备构建数据集
- [x] 更好的评价指标(生成式，检索式)Multi-view
- [x] 添加微信公众号接口
- [x] 发现huggingface/transformers的BertTokenize的bug
- [ ] 测试检索加强的GPT2
- [x] 生成模型sample出来的多个句子可以用来作为检索式模型粗筛的query集合
- [ ] 测试互助学习
- [x] 重新整理Elasticsearch的内容，避免出现类似"图片评论"这类无用的回复
- [x] batch版本的强化学习，并验证reward提升的效果
- [ ] 强化学习的reward函数需不需要gamma考虑远程的奖励，考虑远程的奖励的话可能会带来负面的效应，尝试只使用当前的step的奖励作为最终奖励
- [x] attention mask for all the transformer models
- [ ] MultiView的评价指标在强化学习中会严重的降低速度，可以考虑构建一个模型来评价多个角度，提高速度和效率(一个BERT编码层和多个任务头: 一致性头，兼容性头，流畅性头，安全性头[安全性就是一致性])
- [x] 重新检查 dataloader 里面的截断处理是对context左截短，response右截断
- [x] Multi-view参考一下 AAAI 2020: Learning from Easy to Complex: Adaptive Multi-curricula Learning for Neural Dialogue Generation
- [ ] 生成模型推理阶段不限制长度，知道遇到 [SEP]
- [x] 将一个多轮对话的session全部记录下来，避免检索式对话系统多次选择重复的回复
- [ ] 通过对检索式对话系统进行分析发现，检索式对话系统很容易对一些逻辑错误的候选回复给予非常h高的分数，仅仅是因为出现了和上下文主题类似的单词，比如，“你喜欢什么类型的电影”，“我喜欢打乒乓球”的分数是0.9687，但是"我知道打乒乓球"的分数是0.1595可见，模型根据单词“喜欢”就给候选回复一个非常搞的分数，这是不合理的。
- [x] topk,topp对测试阶段生成的文本质量影响很大，topk越小，搜索空间越小，质量越好
- [x] MultiView也可以加入句子长度的分数模型，避免选择一些例如“真的吗”，“好的”之类的高频无意义词汇，但是也需要注意，长度设置不好的话，很容易倾向于选择长的检索式结果 
- [ ] 借由DialoGPT引发的疑问，中文的高质量对话数据集相对于英文来说比较少，是否可以使用大规模非对话语料增强对话模型的效果
- [ ] 检索式对话系统可以很好的区分正样本和负样本，但是正样本之间的差异性很难区分，基本上认为的正样本都是0.99的判定概率，这没办法区分好的样本的“好的程度”，这也是一个值得做的idea，可以考虑使用GAN的思路(ICLR 2020微软的那个GAN做文本生成的思路)，通过使用对比损失而不是二分类来使得可以对之前认为是正的样本进行重新判定
- [x] 添加 tensorboard 监控运行状态
- [x] 为了加快生成式对话系统的收敛，用GPT2-chitchat预训练模型作为开始
- [x] apex混合精度加速（混合精度并不能加速，这一点在GPT2-Chinese项目中也提到了）
- [ ] 使用数据增强的方法增强检索式方法，有助于生成式二次排序和检索式对话系统，最好的方法就是使用GPT2GAN来做
- [ ] 生成式对话经常会产生一些重复的单词（流畅性很低，这一点在GPT2的结果来体现），这是因为对话本质上并不是简单的语言模型的生成问题，这样的重复生成模型认为是高概率的生成结果，所以需要我们引入其他的机制在高层次上控制生成的对话的结果（比如使用KL散度预测生成的文本的句子）
- [x] 加入repetition惩罚，提高生成质量
- [x] 对Elasticsearch使用filter过滤机制，提高检索模型的Topic准确度，这一点针对于SMP-MCC 2020 比赛设计
- [ ] 设计keywords GPT2生成模型，分为三阶段训练，context; keywords; response，其中keywords极端可以之后用来做强化学习
- [ ] 检验when2talk
- [ ] 检验gpt2retrieval，查看检索式模型是否可以有效增强生成模型的效果，和kwgpt2做对比看看，如果kwgpt2效果更好那就不用gpt2retrieval模型了，因为gpt2retrieval模型在训练之前还需要大量的实践收集对应的检索式对话候选回复用来做先验条件
- [ ] 使用 Micro and Macro Distinct 作为 multiview 的额外评价指标
- [ ] trigger sentence 合并到当前的句子上下文的后面，用来加强topic信息
- [ ] bm25筛选负样本训练检索式对话效果有提升
- [ ] 生成式模型需要添加speaker embedding，保证生成的句子是对上文query的回应，而不是单纯的语言模型采样的结果。这一点再CDial-GPT(LCCC)的论文和代码里面介绍有。
- [x] bertmcf, bertmc 并不比 bertretrieval 好
